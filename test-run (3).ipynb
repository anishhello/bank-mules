{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_sorted2=pd.read_csv('/kaggle/input/cleanset/cleared_file.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#partitioning the data into sequential data and non sequential data\n","seq_cols=[cols for cols in df_sorted2.columns if 'txn' in cols]\n","X_seq=df_sorted2[seq_cols]\n","X_non_seq=df_sorted2.drop(columns=seq_cols)\n","X_non_seq=X_non_seq.drop(columns=['Target','Primary key'])\n","y=df_sorted2['Target']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.over_sampling import SMOTE\n","import numpy as np\n","X_seq_train, X_seq_test, y_train, y_test = train_test_split(X_seq, y, test_size=0.2, random_state=42)\n","\n","# Standardize or normalize features for sequential data\n","scaler_seq = StandardScaler()\n","X_seq_train_scaled = scaler_seq.fit_transform(X_seq_train)\n","X_seq_test_scaled = scaler_seq.transform(X_seq_test)\n","\n","# Reshape the input data for LSTM (assuming each sequence has 100 features)\n","sequence_length = X_seq_train_scaled.shape[1]\n","n_features = 1  # Assuming univariate time series data\n","\n","X_seq_train_reshaped = X_seq_train_scaled.reshape((X_seq_train_scaled.shape[0], sequence_length * n_features))\n","X_seq_test_reshaped = X_seq_test_scaled.reshape((X_seq_test_scaled.shape[0], sequence_length * n_features))\n","\n","# Apply SMOTE to the training set\n","smote = SMOTE(sampling_strategy='auto', random_state=42)\n","X_seq_train_resampled, y_train_resampled = smote.fit_resample(X_seq_train_reshaped, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_non_seq_train, X_non_seq_test, y_train, y_test = train_test_split(X_non_seq, y, test_size=0.2, random_state=42)\n","\n","# Standardize or normalize features for non-sequential data\n","scaler_non_seq = StandardScaler()\n","X_non_seq_train_scaled = scaler_non_seq.fit_transform(X_non_seq_train)\n","X_non_seq_test_scaled = scaler_non_seq.transform(X_non_seq_test)\n","\n","# Apply SMOTE to the training set for handling class imbalance\n","smote = SMOTE(sampling_strategy='auto', random_state=42)\n","X_non_seq_train_resampled, y_train_resampled = smote.fit_resample(X_non_seq_train_scaled, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.optimizers.schedules import ExponentialDecay\n","lr_schedule = ExponentialDecay(\n","    initial_learning_rate=0.01,\n","    decay_rate=0.9,\n","    decay_steps=1000,\n","    staircase=False  # Set to False for continuous decay\n",")\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import precision_score, recall_score, roc_auc_score\n","from imblearn.over_sampling import SMOTE\n","import pandas as pd\n","import numpy as np\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["n_features_seq=1\n","model_seq = tf.keras.Sequential([\n","    tf.keras.layers.LSTM(64, input_shape=(sequence_length, n_features_seq)),\n","    tf.keras.layers.Dropout(0.5),\n","    \n","    tf.keras.layers.Dense(32, activation='relu'),\n","    \n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","#model_seq is a LSTM model to capture the sequential relations and patterns in the transactions\n","model_seq.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(),\n","                                                                         tf.keras.metrics.Recall()])\n","\n","model_seq.summary()  # Print model summary\n","\n","model_seq.fit(X_seq_train_resampled, y_train_resampled, epochs=15, batch_size=32, validation_split=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_non_seq = tf.keras.Sequential([\n","    tf.keras.layers.Dense(64, activation='relu', input_dim=X_non_seq_train_scaled.shape[1]),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(32, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","#model_non_seq to capture the patterns in non sequential data like demographic details, days of acc etc\n","model_non_seq.compile(optimizer='adam', loss='binary_crossentropy',\n","                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()\n","                          ])\n","model_non_seq.fit(X_non_seq_train_resampled, y_train_resampled,epochs=10, batch_size=32, validation_split=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","scaler=StandardScaler()\n","X_seq_scaled=scaler.fit_transform(X_seq)\n","X_non_seq_scaled=scaler.fit_transform(X_non_seq)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_seq_reshaped = X_seq_scaled.reshape((X_seq_scaled.shape[0], sequence_length, n_features_seq))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sq=model_seq.predict(X_seq_reshaped)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nsq=model_non_seq.predict(X_non_seq_scaled)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sq"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nsq"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n","x1=df_sorted2.drop(columns=['Target','Primary key'])\n","y1=df_sorted2['Target']\n","x, y = smote.fit_resample(x1, y1)\n","# Assuming 'data' is your DataFrame, 'X' contains features, and 'y' is the target variable\n","# Split the data into training and testing sets\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","\n","# Create an XGBoost classifier with adjusted hyperparameters\n","model = XGBClassifier(\n","    n_estimators=100,\n","    learning_rate=0.1,\n","    max_depth=3,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    min_child_weight=1,\n","    gamma=0,\n","    random_state=42\n",")\n","\n","# Train the model\n","model.fit(x_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(x_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","\n","# Display the results\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","\n","# Display additional metrics\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","\n","#training a random forest classifier to extract less complex realation out of the dataset , by tuning hyperparameters we restrict overfitting"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_fill=pd.read_csv('/kaggle/input/filldata/filldata.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_fill.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tree=model.predict_proba(x1)[:,1]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sq=sq.ravel()\n","nsq=nsq.ravel()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_final=pd.DataFrame({'seq':sq})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_final['nonseq']=pd.DataFrame(nsq)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_final['tree']=pd.DataFrame(tree)\n","df_final['target']=pd.DataFrame(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_final"]},{"cell_type":"markdown","metadata":{},"source":["rather than depending only on deep models for the classification we are also taking weightage of ensemble methods here, XGBoost classifier. This comes in handy in case the deep model overfits the training dataset and captures noise in the data ,anyways we have taken proper methods  to prevent overfitting . And the models individually performing well with dataset gave away a motivation to include both.Basically if the  deep models are capturing too complex features including noise it may create a problem however the ensemble method does not capture such intricate(possibly noisy) patterns, thus overall decreasing the probability that comes just due to noise."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","#we obtained the weights using a multi-feature linear regression to calculate the optimum weights tested against the target value with gradient descent algorithm\n","\n","a1=0.7 #sequential\n","a2=1.2 #non sequential\n","a3=2.5 #tree\n","df_final['preds']=(a1*df_final['seq']+a2*df_final['nonseq']+a3*df_final['tree'])/(a1+a2+a3)\n","df_final['predsf']=df_final['preds'].apply(lambda x: 1 if x > 0.5 else 0)\n","match = (df_final['predsf'] == df_final['target']).sum()\n","print(match) #checking the number of matching cells "]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4355993,"sourceId":7482842,"sourceType":"datasetVersion"},{"datasetId":4357217,"sourceId":7484691,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
